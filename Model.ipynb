{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTrAV_W5SEia"
      },
      "source": [
        "Isaac Hus <br>\n",
        "Personal Research Project <br>\n",
        "Winter 2024 <br>\n",
        "MNIST NN Model Stealing <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2VHvRfjySQ9r"
      },
      "outputs": [],
      "source": [
        "# Import the required dependencies\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Version:  11.8\n",
            "CUDA devices (GPU's) found:  1\n"
          ]
        }
      ],
      "source": [
        "# Check for CUDA availability\n",
        "# Print the CUDA details\n",
        "print('CUDA Version: ',torch.version.cuda)\n",
        "print('CUDA devices (GPU\\'s) found: ',torch.cuda.device_count())\n",
        "assert(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next codeblock will create a class that outlines the structure of the NN <br>\n",
        "I am using a linear NN with one hidden layer <br>\n",
        "The forward function is used to get the NN output from an input 'x'<br>\n",
        "Model is linear, look into changing to convoluted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_1GotkXCY5SX"
      },
      "outputs": [],
      "source": [
        "# define the NN architecture as a class\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(28 * 28, 64)\n",
        "        self.l2 = nn.Linear(64,64)\n",
        "        self.l3 = nn.Linear(64, 10)\n",
        "        # dropout module with 0.1 drop probability\n",
        "        # helps in preventing overfitting\n",
        "        self.do = nn.Dropout(0.1)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        h1 = nn.functional.relu(self.l1(x))\n",
        "        h2 = nn.functional.relu(self.l2(h1))\n",
        "        do = self.do(h2 + h1)\n",
        "        logits = self.l3(do)\n",
        "        # return an array of logits\n",
        "        return logits\n",
        "model = ResNet().cuda()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimiser using Stocastic Gradient Descent <br>\n",
        "Common and computationally easy optimiser to minimise loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ytvsvP97ZMgb"
      },
      "outputs": [],
      "source": [
        "# optimiser using SGD\n",
        "optimiser = optim.SGD(model.parameters(), lr=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loss function <br>\n",
        "using cross entropy because its a classification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "n0HPO_XvZkDG"
      },
      "outputs": [],
      "source": [
        "#loss\n",
        "loss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load in the dataset <br>\n",
        "I'm using the MNIST dataset that pytourch provides <br>\n",
        "Also splitting to training and validation <br>\n",
        "Where is test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LLnKWBGdSPy",
        "outputId": "1cec9889-96b4-4511-ec9a-2e19dc601808"
      },
      "outputs": [],
      "source": [
        "data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "train, val = random_split(data, [55000, 5000])\n",
        "training_loader = DataLoader(train, batch_size=32)\n",
        "val_loader = DataLoader(val, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training loop, details in comments <br>\n",
        "Most math is done on the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j7CmdDrZu9B",
        "outputId": "506f09f5-a739-485f-feed-3473183df47d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss: 0.85\n",
            "Epoch 1, validation loss: 0.43\n",
            "Epoch 1, validation accuracy: 0.88\n",
            "Epoch 2, train loss: 0.37\n",
            "Epoch 2, validation loss: 0.34\n",
            "Epoch 2, validation accuracy: 0.90\n",
            "Epoch 3, train loss: 0.30\n",
            "Epoch 3, validation loss: 0.29\n",
            "Epoch 3, validation accuracy: 0.91\n",
            "Epoch 4, train loss: 0.26\n",
            "Epoch 4, validation loss: 0.26\n",
            "Epoch 4, validation accuracy: 0.92\n",
            "Epoch 5, train loss: 0.23\n",
            "Epoch 5, validation loss: 0.24\n",
            "Epoch 5, validation accuracy: 0.93\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "\n",
        "# go through the data set 5 times\n",
        "num_epoch = 5\n",
        "for epoch in range(num_epoch):\n",
        "  losses = []\n",
        "\n",
        "  # step through the data set batch by batch\n",
        "  for batch in training_loader:\n",
        "\n",
        "    # get the data and labels\n",
        "    # x is an image\n",
        "    # y is a label\n",
        "    x, y = batch\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    x = x.view(batch_size, -1).cuda()\n",
        "\n",
        "    #1 forward x through model to get logits\n",
        "    logit = model(x)\n",
        "\n",
        "    #2 compute loss\n",
        "    objective = loss(logit, y.cuda())\n",
        "\n",
        "    #3 cleaning gradient\n",
        "    model.zero_grad()\n",
        "\n",
        "    #4 back propagation to compute gradient\n",
        "    objective.backward()\n",
        "\n",
        "    #5 apply updates (step up gradients)\n",
        "    optimiser.step()\n",
        "\n",
        "\n",
        "    losses.append(objective.item())\n",
        "\n",
        "\n",
        "  print(f'Epoch {epoch + 1}, train loss: {torch.tensor(losses).mean():.2f}')\n",
        "\n",
        "  losses = []\n",
        "  accuracy = []\n",
        "  for batch in val_loader:\n",
        "    x, y = batch\n",
        "    batch_size = x.size(0)\n",
        "    x = x.view(batch_size, -1).cuda()\n",
        "\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      logit = model(x)\n",
        "    objective = loss(logit, y.cuda())\n",
        "    losses.append(objective.item())\n",
        "    accuracy.append(y.eq(logit.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "  print(f'Epoch {epoch + 1}, validation loss: {torch.tensor(losses).mean():.2f}')\n",
        "  print(f'Epoch {epoch + 1}, validation accuracy: {torch.tensor(accuracy).mean():.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'target.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Motivation for study <br>\n",
        "Many current model stealing attacks involve some sort of data poisoning. They aim to interfeir with a possible shadow model while keeping the normal public from getting wrong answers. The feild right now is trying to optimose the ratio between high defence and low error. I plan to explore how model stealing attacks work and then to analys different non-compromising defences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Refrence Material #\n",
        "https://www.frontiersin.org/articles/10.3389/fdata.2021.729663/full  (Look at backdoor watermarking)<br>\n",
        "https://openaccess.thecvf.com/content_CVPR_2020/papers/Kariyappa_Defending_Against_Model_Stealing_Attacks_With_Adaptive_Misinformation_CVPR_2020_paper.pdf <br>\n",
        "https://ieeexplore.ieee.org/abstract/document/8806737"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
